{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from collections import Counter\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "DIR = Path('../data/tweet-sentiment-extraction')\n",
    "df_train = pd.read_csv(DIR / 'train.csv')\n",
    "df_test = pd.read_csv(DIR / 'test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(lambda x: str(x))\n",
    "df_test['text'] = df_test['text'].apply(lambda x: str(x))\n",
    "df_train['uncased_text'] = df_train['text'].apply(lambda x: x.lower())\n",
    "df_test['uncased_text'] = df_test['text'].apply(lambda x: x.lower())\n",
    "df_train['selected_text'] = df_train['selected_text'].apply(lambda x: str(x).lower())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "TOKENIZER_DIR = Path('../tokenizers')\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_DIR / 'berttokenizer-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "df_train['tokenized_text'] = df_train['uncased_text'].apply(tokenizer.tokenize)\n",
    "df_test['tokenized_text'] = df_test['uncased_text'].apply(tokenizer.tokenize)\n",
    "df_train['tokenized_selected_text'] = df_train['selected_text'].apply(tokenizer.tokenize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n\n                         selected_text sentiment  \\\n0  i`d have responded, if i were going   neutral   \n1                             sooo sad  negative   \n2                          bullying me  negative   \n3                       leave me alone  negative   \n4                        sons of ****,  negative   \n\n                                        uncased_text  \\\n0                i`d have responded, if i were going   \n1      sooo sad i will miss you here in san diego!!!   \n2                          my boss is bullying me...   \n3                     what interview! leave me alone   \n4   sons of ****, why couldn`t they put them on t...   \n\n                                      tokenized_text  \\\n0  [i, `, d, have, responded, ,, if, i, were, going]   \n1  [soo, ##o, sad, i, will, miss, you, here, in, ...   \n2              [my, boss, is, bullying, me, ., ., .]   \n3             [what, interview, !, leave, me, alone]   \n4  [sons, of, *, *, *, *, ,, why, couldn, `, t, t...   \n\n                             tokenized_selected_text  \n0  [i, `, d, have, responded, ,, if, i, were, going]  \n1                                    [soo, ##o, sad]  \n2                                     [bullying, me]  \n3                                 [leave, me, alone]  \n4                          [sons, of, *, *, *, *, ,]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n      <th>uncased_text</th>\n      <th>tokenized_text</th>\n      <th>tokenized_selected_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>i`d have responded, if i were going</td>\n      <td>neutral</td>\n      <td>i`d have responded, if i were going</td>\n      <td>[i, `, d, have, responded, ,, if, i, were, going]</td>\n      <td>[i, `, d, have, responded, ,, if, i, were, going]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549e992a42</td>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>sooo sad</td>\n      <td>negative</td>\n      <td>sooo sad i will miss you here in san diego!!!</td>\n      <td>[soo, ##o, sad, i, will, miss, you, here, in, ...</td>\n      <td>[soo, ##o, sad]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>088c60f138</td>\n      <td>my boss is bullying me...</td>\n      <td>bullying me</td>\n      <td>negative</td>\n      <td>my boss is bullying me...</td>\n      <td>[my, boss, is, bullying, me, ., ., .]</td>\n      <td>[bullying, me]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9642c003ef</td>\n      <td>what interview! leave me alone</td>\n      <td>leave me alone</td>\n      <td>negative</td>\n      <td>what interview! leave me alone</td>\n      <td>[what, interview, !, leave, me, alone]</td>\n      <td>[leave, me, alone]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>sons of ****,</td>\n      <td>negative</td>\n      <td>sons of ****, why couldn`t they put them on t...</td>\n      <td>[sons, of, *, *, *, *, ,, why, couldn, `, t, t...</td>\n      <td>[sons, of, *, *, *, *, ,]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1487280/4155114754.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(range(len(df_train))):\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/27481 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f9f45460ec64c3e81278ab1beee57cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter train data\n",
    "# 开始结束词\n",
    "start_position_candidates = []\n",
    "end_position_candidates = []\n",
    "df_train['select_length'] = df_train['tokenized_selected_text'].map(len)\n",
    "\n",
    "for i in tqdm(range(len(df_train))):\n",
    "    start_position_candidate = [j for j, tok in enumerate(df_train['tokenized_text'].iloc[i]) if\n",
    "                                tok == df_train['tokenized_selected_text'].iloc[i][0]]\n",
    "    end_position_candidate = [j for j, tok in enumerate(df_train['tokenized_text'].iloc[i]) if\n",
    "                              tok == df_train['tokenized_selected_text'].iloc[i][-1]]\n",
    "\n",
    "    start_position_candidate = [idx for idx in start_position_candidate if\n",
    "                                idx + df_train['select_length'].iloc[i] - 1 in end_position_candidate]\n",
    "    end_position_candidate = [idx for idx in end_position_candidate if\n",
    "                              idx - df_train['select_length'].iloc[i] + 1 in start_position_candidate]\n",
    "\n",
    "    start_position_candidates.append(start_position_candidate)\n",
    "    end_position_candidates.append(end_position_candidate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# 如果存在多个候选词，取第一个\n",
    "start_position_candidates = [l[0] if len(l) > 0 else -1 for l in start_position_candidates]\n",
    "end_position_candidates = [l[0] if len(l) > 0 else -1 for l in end_position_candidates]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# 测试集合中的开始和结束位置置空 -1\n",
    "df_train['start_position'] = start_position_candidates\n",
    "df_train['end_position'] = end_position_candidates\n",
    "df_test['start_position'] = -1\n",
    "df_test['end_position'] = -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "df_train = df_train.query('start_position!=-1')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df_train, train_size=0.8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "pos_train = df_train.query('sentiment==\"positive\"')\n",
    "neg_train = df_train.query('sentiment==\"negative\"')\n",
    "neu_train = df_train.query('sentiment==\"neutral\"')\n",
    "\n",
    "pos_val = df_train.query('sentiment==\"positive\"')\n",
    "neg_val = df_train.query('sentiment==\"negative\"')\n",
    "neu_val = df_train.query('sentiment==\"neutral\"')\n",
    "\n",
    "pos_test = df_test.query('sentiment==\"positive\"')\n",
    "neg_test = df_test.query('sentiment==\"negative\"')\n",
    "neu_test = df_test.query('sentiment==\"neutral\"')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "MODEL_DIR = Path('../models/bertforquestionanswering-base-uncased')\n",
    "pos_model = BertForQuestionAnswering.from_pretrained(MODEL_DIR)\n",
    "neg_model = BertForQuestionAnswering.from_pretrained(MODEL_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.texts = df['uncased_text'].values\n",
    "        self.hash_index = df['textID'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        returns = {\n",
    "            'text': self.texts[idx],\n",
    "            'idx': idx\n",
    "        }\n",
    "        return returns\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.texts = df['uncased_text'].values\n",
    "        self.start_ids = df['start_position'].values\n",
    "        self.end_ids = df['end_position'].values\n",
    "        self.hash_index = df['textID'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        returns = {\n",
    "            'text': self.texts[idx],\n",
    "            'start': self.start_ids[idx],\n",
    "            'end': self.end_ids[idx],\n",
    "            'idx': idx\n",
    "        }\n",
    "        return returns\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df_train, df_val, df_test):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(TrainDataset(df_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(TrainDataset(df_val), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(TestDataset(df_test), batch_size=BATCH_SIZE, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "class BaseSuperModule(pl.LightningModule):\n",
    "    def __init__(self, bertmodel, tokenizer, prediction_save_path):\n",
    "        super().__init__()\n",
    "        self.bertmodel = bertmodel\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prediction_save_path = prediction_save_path\n",
    "\n",
    "    def get_device(self):\n",
    "        return self.bertmodel.state_dict()['bert.embeddings.word_embeddings.weight'].device\n",
    "\n",
    "    def save_predictions(self, start_positions, end_positions):\n",
    "        d = pd.DataFrame({'start_position': start_positions, 'end_position': end_positions})\n",
    "        d.to_csv(self.prediction_save_path, index=False)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            batch(dict), where\n",
    "                batch['text'] = uncased text: str\n",
    "                batch['idx'] = raw text: list(int)\n",
    "                batch['start'] = start position indices : list(int) (for train & val batch only)\n",
    "                batch['end'] = end position indices : list(int) (for train & val batch only)\n",
    "\n",
    "        Output:\n",
    "            For train batch, which has 'start' key and 'end' key:\n",
    "                Tuple of (loss(int), start_score(torch.tensor), end_score(torch.tensor))\n",
    "            For test batch, without 'start' key and 'end' key:\n",
    "                Tuple of (start_score(torch.tensor), end_score(torch.tensor))\n",
    "        \"\"\"\n",
    "        encoded_batch = tokenizer.batch_encode_plus(batch['text'], max_length=MAX_LENGTH, pad_to_max_length=True)\n",
    "        input_ids = torch.tensor(encoded_batch['input_ids']).to(self.get_device())\n",
    "        attention_mask = torch.tensor(encoded_batch['attention_mask']).to(self.get_device())\n",
    "        start_positions = batch['start'].to(self.get_device()) + 1 if 'start' in batch.keys() else None\n",
    "        end_positions = batch['end'].to(self.get_device()) + 1 if 'end' in batch.keys() else None\n",
    "\n",
    "        model_inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'start_positions': start_positions,\n",
    "            'end_positions': end_positions\n",
    "        }\n",
    "\n",
    "        return self.bertmodel(**model_inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \"\"\"\n",
    "        (batch) -> (dict or OrderedDict)\n",
    "        # Caution: key for loss function must exactly be 'loss'.\n",
    "        \"\"\"\n",
    "        idx = batch['idx']\n",
    "        loss = self.forward(batch)[0]\n",
    "        return {'loss': loss, 'idx': idx}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        \"\"\"\n",
    "        (batch) -> (dict or OrderedDict)\n",
    "        # Caution: key for loss function must exactly be 'loss'.\n",
    "        \"\"\"\n",
    "        idx = batch['idx']\n",
    "        loss = self.forward(batch)[0]\n",
    "        return {'loss': loss, 'idx': idx}\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        \"\"\"\n",
    "        (batch) -> (dict or OrderedDict)\n",
    "        \"\"\"\n",
    "        idx = batch['idx']\n",
    "        start_scores = self.forward(batch)[0]\n",
    "        end_scores = self.forward(batch)[1]\n",
    "        return {'start_scores': start_scores, 'end_scores': end_scores, 'idx': idx}\n",
    "\n",
    "    def training_end(self, outputs):\n",
    "        \"\"\"\n",
    "        outputs(dict) -> loss(dict or OrderedDict)\n",
    "        # Caution: key must exactly be 'loss'.\n",
    "        \"\"\"\n",
    "        return {'loss': outputs['loss']}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        \"\"\"\n",
    "        For single dataloader:\n",
    "            outputs(list of dict) -> (dict or OrderedDict)\n",
    "        For multiple dataloaders:\n",
    "            outputs(list of (list of dict)) -> (dict or OrderedDict)\n",
    "        \"\"\"\n",
    "        return {'loss': torch.mean(torch.tensor([output['loss'] for output in outputs])).detach()}\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        \"\"\"\n",
    "        For single dataloader:\n",
    "            outputs(list of dict) -> (dict or OrderedDict)\n",
    "        For multiple dataloaders:\n",
    "            outputs(list of (list of dict)) -> (dict or OrderedDict)\n",
    "        \"\"\"\n",
    "        start_scores = torch.cat([output['start_scores'] for output in outputs]).detach().cpu().numpy()\n",
    "        start_positions = np.argmax(start_scores, axis=1) - 1\n",
    "\n",
    "        end_scores = torch.cat([output['end_scores'] for output in outputs]).detach().cpu().numpy()\n",
    "        end_positions = np.argmax(end_scores, axis=1) - 1\n",
    "        self.save_predictions(start_positions, end_positions)\n",
    "        return {}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=2e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "pos_module = BaseSuperModule(pos_model, tokenizer, 'pos_pred.csv')\n",
    "neg_module = BaseSuperModule(neg_model, tokenizer, 'neg_pred.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "BaseSuperModule(\n  (bertmodel): BertForQuestionAnswering(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (6): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (7): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (8): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (9): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (10): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (11): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_module.to(device)\n",
    "neg_module.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/seeyou/anaconda3/envs/pyg/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1812: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "DEBUG_MODE = False\n",
    "pos_trainer = pl.Trainer(max_epochs=3, fast_dev_run=DEBUG_MODE)\n",
    "neg_trainer = pl.Trainer(max_epochs=3, fast_dev_run=DEBUG_MODE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/seeyou/projects/模板/lightning-hydra-template/notebooks/lightning_logs\n",
      "\n",
      "  | Name      | Type                     | Params\n",
      "-------------------------------------------------------\n",
      "0 | bertmodel | BertForQuestionAnswering | 108 M \n",
      "-------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "435.573   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f25bdf30f03496bb8daebb6fe517a14"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seeyou/anaconda3/envs/pyg/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/seeyou/anaconda3/envs/pyg/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/seeyou/anaconda3/envs/pyg/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a87f1e25a21b400294e115882ca667a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seeyou/anaconda3/envs/pyg/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "pos_trainer.fit(pos_module, datamodule=DataModule(pos_train, pos_val, pos_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neg_trainer.fit(neg_module, datamodule=DataModule(neg_train, neg_val, neg_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/kswamy15/pytorch-lightning-imdb-bert/blob/master/Bert_NLP_Pytorch_IMDB_v3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "import transformers\n",
    "from nlp import load_dataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from argparse import ArgumentParser\n",
    "import re\n",
    "from typing import Optional\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# custom dataset uses Bert Tokenizer to create the Pytorch Dataset\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, notes, targets, tokenizer, max_len):\n",
    "        self.notes = notes\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.notes))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        note = str(self.notes[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            note,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            #'text': note,\n",
    "            'label': torch.tensor(target, dtype=torch.long),\n",
    "            'input_ids': (encoding['input_ids']).flatten(),\n",
    "            'attention_mask': (encoding['attention_mask']).flatten(),\n",
    "            'token_type_ids': (encoding['token_type_ids']).flatten()\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "t = transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[101, 7592, 2026, 2814, 102, 2023, 2003, 1037, 3899, 102]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.encode('hello my friends', 'this is a dog')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: str,\n",
    "                 tokenizer: str = 'bert-base-uncased',\n",
    "                 max_len: int = 500,\n",
    "                 batch_size: int = 64,\n",
    "                 num_workers: int = 4,\n",
    "                 *args,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.tokenizer = transformers.BertTokenizerFast.from_pretrained(self.hparams.tokenizer)\n",
    "\n",
    "        self.REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "        self.NO_SPACE = \"\"\n",
    "        self.SPACE = \" \"\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        ## Creates a list of reviews from the big text files containing all the reviews\n",
    "        ## This code was taken from here https://github.com/aaronkub/machine-learning-examples/blob/master/imdb-sentiment-analysis/Sentiment%20Analysis%20Walkthrough%20Part%201.ipynb\n",
    "        reviews_train = []\n",
    "        DIR = Path(self.hparams.data_dir)\n",
    "        with open(DIR / 'full_train.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                reviews_train.append(line.strip())\n",
    "\n",
    "        reviews_test = []\n",
    "        with open(DIR / 'full_test.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                reviews_test.append(line.strip())\n",
    "\n",
    "        self.reviews_train_clean = self.preprocess_reviews(reviews_train)\n",
    "        self.reviews_test_clean = self.preprocess_reviews(reviews_test)\n",
    "\n",
    "    def preprocess_reviews(self, reviews):\n",
    "        #reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "        reviews = [self.REPLACE_WITH_SPACE.sub(self.SPACE, line) for line in reviews]\n",
    "\n",
    "        return reviews\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        ## Creating dataframes from the list data.  The reviews are arranged in the order that the first 12,500 belong to positive reviews and the rest 12,500 belong to negative reviews.\n",
    "        df_train_reviews_clean = pd.DataFrame(self.reviews_train_clean, columns=['reviews'])\n",
    "        df_train_reviews_clean['target'] = np.where(df_train_reviews_clean.index < 12500, 1, 0)\n",
    "\n",
    "        df_test_reviews_clean = pd.DataFrame(self.reviews_test_clean, columns=['reviews'])\n",
    "        df_test_reviews_clean['target'] = np.where(df_test_reviews_clean.index < 12500, 1, 0)\n",
    "\n",
    "        # Shuffling the rows in both the train and test data.  This is very important before using the data for training.\n",
    "        df_train_reviews_clean = df_train_reviews_clean.sample(frac=1).reset_index(drop=True)\n",
    "        df_test_reviews_clean = df_test_reviews_clean.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # breaking the train data into training and validation\n",
    "        df_train, df_valid = train_test_split(df_train_reviews_clean, test_size=0.25,\n",
    "                                              stratify=df_train_reviews_clean['target'])\n",
    "\n",
    "        self.train = df_train.reset_index(drop=True)\n",
    "        self.val = df_valid.reset_index(drop=True)\n",
    "        self.test = df_test_reviews_clean\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(ImdbDataset(notes=self.train['reviews'],\n",
    "                                      targets=self.train['target'],\n",
    "                                      tokenizer=self.tokenizer,\n",
    "                                      max_len=self.hparams.max_len\n",
    "                                      ),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(ImdbDataset(notes=self.val['reviews'],\n",
    "                                      targets=self.val['target'],\n",
    "                                      tokenizer=self.tokenizer,\n",
    "                                      max_len=self.hparams.max_len),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(ImdbDataset(notes=self.test['reviews'],\n",
    "                                      targets=self.test['target'],\n",
    "                                      tokenizer=self.tokenizer,\n",
    "                                      max_len=self.hparams.max_len),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## The main Pytorch Lightning module\n",
    "class ImdbModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate: float = 0.0001 * 8,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.num_labels = 2\n",
    "        config = transformers.DistilBertConfig(dropout=0.1, attention_dropout=0.2)\n",
    "        self.bert = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "        self.pre_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels)\n",
    "        self.dropout = torch.nn.Dropout(self.bert.config.seq_classif_dropout)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "\n",
    "        hidden_state = outputs[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.relu(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def get_outputs(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        hidden_state = outputs[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        return pooled_output\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # batch\n",
    "        input_ids = batch['input_ids']\n",
    "        label = batch['label']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #token_type_ids = batch['token_type_ids']\n",
    "        # fwd\n",
    "        y_hat = self(input_ids, attention_mask, label)\n",
    "\n",
    "        # loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(y_hat.view(-1, self.num_labels), label.view(-1))\n",
    "        #loss = F.cross_entropy(y_hat, label)\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'train_loss': loss, 'learn_rate': self.optim.param_groups[0]['lr']}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # batch\n",
    "        input_ids = batch['input_ids']\n",
    "        label = batch['label']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #token_type_ids = batch['token_type_ids']\n",
    "        # fwd\n",
    "        y_hat = self(input_ids, attention_mask, label)\n",
    "\n",
    "        # loss\n",
    "        #loss = F.cross_entropy(y_hat, label)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(y_hat.view(-1, self.num_labels), label.view(-1))\n",
    "\n",
    "        # acc\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        val_acc = accuracy_score(y_hat.cpu(), label.cpu())\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'val_loss': loss, 'val_acc': val_acc}\n",
    "        # can't log in validation step lossess, accuracy.  It wouldn't log it at every validation step\n",
    "        return {'val_loss': loss, 'val_acc': val_acc, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'val_loss': avg_loss, 'val_acc': avg_val_acc}\n",
    "        return {'val_loss': avg_loss, 'progress_bar': tensorboard_logs, 'log': tensorboard_logs}\n",
    "\n",
    "    def on_batch_end(self):\n",
    "        #for group in self.optim.param_groups:\n",
    "        #    print('learning rate', group['lr'])\n",
    "        # This is needed to use the One Cycle learning rate that needs the learning rate to change after every batch\n",
    "        # Without this, the learning rate will only change after every epoch\n",
    "        if self.sched is not None:\n",
    "            self.sched.step()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.sched is not None:\n",
    "            self.sched.step()\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        input_ids = batch['input_ids']\n",
    "        label = batch['label']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #token_type_ids = batch['token_type_ids']\n",
    "        y_hat = self(input_ids, attention_mask, label)\n",
    "\n",
    "        # loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(y_hat.view(-1, self.num_labels), label.view(-1))\n",
    "\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        test_acc = accuracy_score(y_hat.cpu(), label.cpu())\n",
    "\n",
    "        return {'test_loss': loss, 'test_acc': torch.tensor(test_acc)}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_test_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {'avg_test_loss': avg_loss, 'avg_test_acc': avg_test_acc}\n",
    "        return {'avg_test_acc': avg_test_acc, 'log': tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    # ---------------------\n",
    "    # TRAINING SETUP\n",
    "    # ---------------------\n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        # (LBFGS it is automatically supported, no need for closure function)\n",
    "        optimizer = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.hparams.learning_rate,\n",
    "                                     eps=1e-08)\n",
    "        #scheduler = StepLR(optimizer, step_size=1, gamma=0.2)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-5, total_steps=2000)\n",
    "        #scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-7, max_lr=1e-4, cycle_momentum=False,step_size_up=300)\n",
    "\n",
    "        #scheduler = ReduceLROnPlateau(optimizer, patience=0, factor=0.2)\n",
    "        self.sched = scheduler\n",
    "        self.optim = optimizer\n",
    "        return [optimizer], [scheduler]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tb_logger = pl_loggers.TensorBoardLogger('../.cache/logs/')\n",
    "\n",
    "# callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode='min',\n",
    "    strict=True,\n",
    ")\n",
    "\n",
    "#print ('inside checkpoint loop')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    #filepath='best_model_{epoch:02d}-{val_loss:.2f}',\n",
    "    dirpath='../.cache/best_model',\n",
    "    verbose=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "lr_logger = LearningRateMonitor(logging_interval='epoch')\n",
    "trainer = Trainer(logger=tb_logger, callbacks=[checkpoint, lr_logger, early_stop], accelerator='gpu', devices=1, max_epochs=3)\n",
    "\n",
    "model = ImdbModel()\n",
    "datamodule = DataModule('../data/imdb/movie_data', batch_size=64)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "trainer.fit(model, datamodule=datamodule)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('zsl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1efd3c77b126e853d7107796dada0eb58262837b04201097653767f94791c600"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
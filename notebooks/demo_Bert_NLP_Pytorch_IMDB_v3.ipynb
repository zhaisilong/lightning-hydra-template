{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/kswamy15/pytorch-lightning-imdb-bert/blob/master/Bert_NLP_Pytorch_IMDB_v3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "import transformers\n",
    "from nlp import load_dataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from argparse import ArgumentParser\n",
    "import re\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# custom dataset uses Bert Tokenizer to create the Pytorch Dataset\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, notes, targets, tokenizer, max_len):\n",
    "        self.notes = notes\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.notes))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        note = str(self.notes[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            note,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            #'text': note,\n",
    "            'label': torch.tensor(target, dtype=torch.long),\n",
    "            'input_ids': (encoding['input_ids']).flatten(),\n",
    "            'attention_mask': (encoding['attention_mask']).flatten(),\n",
    "            'token_type_ids': (encoding['token_type_ids']).flatten()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 24.2kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:01<00:00, 227kB/s]  \n",
      "Downloading: 100%|██████████| 455k/455k [00:01<00:00, 345kB/s]  \n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 484kB/s]\n"
     ]
    }
   ],
   "source": [
    "t = transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: str,\n",
    "                 tokenizer: str = 'bert-base-uncased',\n",
    "                 max_len: int = 500,\n",
    "                 batch_size: int = 64,\n",
    "                 num_workers: int = 4,\n",
    "                 *args,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.tokenizer = transformers.BertTokenizerFast.from_pretrained(self.hparams.tokenizer)\n",
    "\n",
    "        self.REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "        self.NO_SPACE = \"\"\n",
    "        self.SPACE = \" \"\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        ## Creates a list of reviews from the big text files containing all the reviews\n",
    "        ## This code was taken from here https://github.com/aaronkub/machine-learning-examples/blob/master/imdb-sentiment-analysis/Sentiment%20Analysis%20Walkthrough%20Part%201.ipynb\n",
    "        reviews_train = []\n",
    "        DIR = Path(self.hparams.data_dir)\n",
    "        with open(DIR / 'full_train.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                reviews_train.append(line.strip())\n",
    "\n",
    "        reviews_test = []\n",
    "        with open(DIR / 'full_test.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                reviews_test.append(line.strip())\n",
    "\n",
    "        self.reviews_train_clean = self.preprocess_reviews(reviews_train)\n",
    "        self.reviews_test_clean = self.preprocess_reviews(reviews_test)\n",
    "\n",
    "    def preprocess_reviews(self, reviews):\n",
    "        #reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "        reviews = [self.REPLACE_WITH_SPACE.sub(self.SPACE, line) for line in reviews]\n",
    "\n",
    "        return reviews\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        ## Creating dataframes from the list data.  The reviews are arranged in the order that the first 12,500 belong to positive reviews and the rest 12,500 belong to negative reviews.\n",
    "        df_train_reviews_clean = pd.DataFrame(self.reviews_train_clean, columns=['reviews'])\n",
    "        df_train_reviews_clean['target'] = np.where(df_train_reviews_clean.index < 12500, 1, 0)\n",
    "\n",
    "        df_test_reviews_clean = pd.DataFrame(self.reviews_test_clean, columns=['reviews'])\n",
    "        df_test_reviews_clean['target'] = np.where(df_test_reviews_clean.index < 12500, 1, 0)\n",
    "\n",
    "        # Shuffling the rows in both the train and test data.  This is very important before using the data for training.\n",
    "        df_train_reviews_clean = df_train_reviews_clean.sample(frac=1).reset_index(drop=True)\n",
    "        df_test_reviews_clean = df_test_reviews_clean.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # breaking the train data into training and validation\n",
    "        df_train, df_valid = train_test_split(df_train_reviews_clean, test_size=0.25,\n",
    "                                              stratify=df_train_reviews_clean['target'])\n",
    "\n",
    "        self.train = df_train.reset_index(drop=True)\n",
    "        self.val = df_valid.reset_index(drop=True)\n",
    "        self.test = df_test_reviews_clean\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(ImdbDataset(notes=self.train['reviews'],\n",
    "                                      targets=self.train['target'],\n",
    "                                      tokenizer=self.tokenizer,\n",
    "                                      max_len=self.hparams.max_len\n",
    "                                      ),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(ImdbDataset(notes=self.val['reviews'],\n",
    "                                      targets=self.val['target'],\n",
    "                                      tokenizer=self.tokenizer,\n",
    "                                      max_len=self.hparams.max_len),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(ImdbDataset(notes=self.test['reviews'],\n",
    "                                      targets=self.test['target'],\n",
    "                                      tokenizer=self.tokenizer,\n",
    "                                      max_len=self.hparams.max_len),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## The main Pytorch Lightning module\n",
    "class ImdbModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate: float = 0.0001 * 8,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.num_labels = 2\n",
    "        config = transformers.DistilBertConfig(dropout=0.1, attention_dropout=0.2, n_layers=1, n_heads=2)\n",
    "        self.bert = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "        self.pre_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels)\n",
    "        self.dropout = torch.nn.Dropout(self.bert.config.seq_classif_dropout)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "\n",
    "        hidden_state = outputs[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.relu(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def get_outputs(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        hidden_state = outputs[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        return pooled_output\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # batch\n",
    "        input_ids = batch['input_ids']\n",
    "        label = batch['label']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #token_type_ids = batch['token_type_ids']\n",
    "        # fwd\n",
    "        y_hat = self(input_ids, attention_mask, label)\n",
    "\n",
    "        # loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(y_hat.view(-1, self.num_labels), label.view(-1))\n",
    "        #loss = F.cross_entropy(y_hat, label)\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'train_loss': loss, 'learn_rate': self.optim.param_groups[0]['lr']}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # batch\n",
    "        input_ids = batch['input_ids']\n",
    "        label = batch['label']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #token_type_ids = batch['token_type_ids']\n",
    "        # fwd\n",
    "        y_hat = self(input_ids, attention_mask, label)\n",
    "\n",
    "        # loss\n",
    "        #loss = F.cross_entropy(y_hat, label)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(y_hat.view(-1, self.num_labels), label.view(-1))\n",
    "\n",
    "        # acc\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        val_acc = accuracy_score(y_hat.cpu(), label.cpu())\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'val_loss': loss, 'val_acc': val_acc}\n",
    "        # can't log in validation step lossess, accuracy.  It wouldn't log it at every validation step\n",
    "        return {'val_loss': loss, 'val_acc': val_acc, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'val_loss': avg_loss, 'val_acc': avg_val_acc}\n",
    "        return {'val_loss': avg_loss, 'progress_bar': tensorboard_logs, 'log': tensorboard_logs}\n",
    "\n",
    "    def on_batch_end(self):\n",
    "        #for group in self.optim.param_groups:\n",
    "        #    print('learning rate', group['lr'])\n",
    "        # This is needed to use the One Cycle learning rate that needs the learning rate to change after every batch\n",
    "        # Without this, the learning rate will only change after every epoch\n",
    "        if self.sched is not None:\n",
    "            self.sched.step()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.sched is not None:\n",
    "            self.sched.step()\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        input_ids = batch['input_ids']\n",
    "        label = batch['label']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #token_type_ids = batch['token_type_ids']\n",
    "        y_hat = self(input_ids, attention_mask, label)\n",
    "\n",
    "        # loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(y_hat.view(-1, self.num_labels), label.view(-1))\n",
    "\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        test_acc = accuracy_score(y_hat.cpu(), label.cpu())\n",
    "\n",
    "        return {'test_loss': loss, 'test_acc': torch.tensor(test_acc)}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_test_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {'avg_test_loss': avg_loss, 'avg_test_acc': avg_test_acc}\n",
    "        return {'avg_test_acc': avg_test_acc, 'log': tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    # ---------------------\n",
    "    # TRAINING SETUP\n",
    "    # ---------------------\n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        # (LBFGS it is automatically supported, no need for closure function)\n",
    "        optimizer = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.hparams.learning_rate,\n",
    "                                     eps=1e-08)\n",
    "        #scheduler = StepLR(optimizer, step_size=1, gamma=0.2)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-5, total_steps=2000)\n",
    "        #scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-7, max_lr=1e-4, cycle_momentum=False,step_size_up=300)\n",
    "\n",
    "        #scheduler = ReduceLROnPlateau(optimizer, patience=0, factor=0.2)\n",
    "        self.sched = scheduler\n",
    "        self.optim = optimizer\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Downloading: 100%|██████████| 256M/256M [01:12<00:00, 3.70MB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['distilbert.transformer.layer.4.attention.q_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/imdb/imdb/movie_data/full_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000009vscode-remote?line=24'>25</a>\u001b[0m model \u001b[39m=\u001b[39m ImdbModel()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000009vscode-remote?line=25'>26</a>\u001b[0m datamodule \u001b[39m=\u001b[39m DataModule(\u001b[39m'\u001b[39m\u001b[39m../data/imdb/imdb/movie_data\u001b[39m\u001b[39m'\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000009vscode-remote?line=26'>27</a>\u001b[0m datamodule\u001b[39m.\u001b[39;49mprepare_data()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000009vscode-remote?line=27'>28</a>\u001b[0m datamodule\u001b[39m.\u001b[39msetup()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000009vscode-remote?line=28'>29</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(model, datamodule\u001b[39m=\u001b[39mdatamodule)\n",
      "\u001b[1;32m/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb Cell 6'\u001b[0m in \u001b[0;36mDataModule.prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000005vscode-remote?line=25'>26</a>\u001b[0m reviews_train \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000005vscode-remote?line=26'>27</a>\u001b[0m DIR \u001b[39m=\u001b[39m Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mdata_dir)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000005vscode-remote?line=27'>28</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(DIR \u001b[39m/\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mfull_train.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000005vscode-remote?line=28'>29</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000005vscode-remote?line=29'>30</a>\u001b[0m         reviews_train\u001b[39m.\u001b[39mappend(line\u001b[39m.\u001b[39mstrip())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/imdb/imdb/movie_data/full_train.txt'"
     ]
    }
   ],
   "source": [
    "tb_logger = pl_loggers.TensorBoardLogger('../.cache/logs/')\n",
    "\n",
    "# callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode='min',\n",
    "    strict=True,\n",
    ")\n",
    "\n",
    "#print ('inside checkpoint loop')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    #filepath='best_model_{epoch:02d}-{val_loss:.2f}',\n",
    "    dirpath='../.cache/best_model',\n",
    "    verbose=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "lr_logger = LearningRateMonitor(logging_interval='epoch')\n",
    "trainer = Trainer(logger=tb_logger, callbacks=[checkpoint, lr_logger, early_stop], accelerator='gpu', devices=1, max_epochs=10)\n",
    "\n",
    "model = ImdbModel()\n",
    "datamodule = DataModule('../data/imdb/imdb/movie_data', batch_size=12)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a530f459ba6cd60adca24892bd2847578451366d11b8bf0a570eeacab4bc5b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

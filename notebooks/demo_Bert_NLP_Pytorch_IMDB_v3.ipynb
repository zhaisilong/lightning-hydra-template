{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/kswamy15/pytorch-lightning-imdb-bert/blob/master/Bert_NLP_Pytorch_IMDB_v3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "import transformers\n",
    "from nlp import load_dataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from argparse import ArgumentParser\n",
    "import re\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# custom dataset uses Bert Tokenizer to create the Pytorch Dataset\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, notes, targets, tokenizer, max_len):\n",
    "        self.notes = notes\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.notes))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        note = str(self.notes[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            note,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            #'text': note,\n",
    "            'label': torch.tensor(target, dtype=torch.long),\n",
    "            'input_ids': (encoding['input_ids']).flatten(),\n",
    "            'attention_mask': (encoding['attention_mask']).flatten(),\n",
    "            'token_type_ids': (encoding['token_type_ids']).flatten()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t = transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: str,\n",
    "                 tokenizer: str = 'bert-base-uncased',\n",
    "                 max_len: int = 500,\n",
    "                 batch_size: int = 64,\n",
    "                 num_workers: int = 4,\n",
    "                 *args,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.tokenizer = transformers.BertTokenizerFast.from_pretrained(self.hparams.tokenizer)\n",
    "\n",
    "        self.REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "        self.NO_SPACE = \"\"\n",
    "        self.SPACE = \" \"\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        ## Creates a list of reviews from the big text files containing all the reviews\n",
    "        ## This code was taken from here https://github.com/aaronkub/machine-learning-examples/blob/master/imdb-sentiment-analysis/Sentiment%20Analysis%20Walkthrough%20Part%201.ipynb\n",
    "        reviews_train = []\n",
    "        DIR = Path(self.hparams.data_dir)\n",
    "        with open(DIR / 'full_train.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                reviews_train.append(line.strip())\n",
    "\n",
    "        reviews_test = []\n",
    "        with open(DIR / 'full_test.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                reviews_test.append(line.strip())\n",
    "\n",
    "        self.reviews_train_clean = self.preprocess_reviews(reviews_train)\n",
    "        self.reviews_test_clean = self.preprocess_reviews(reviews_test)\n",
    "\n",
    "    def preprocess_reviews(self, reviews):\n",
    "        #reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "        reviews = [self.REPLACE_WITH_SPACE.sub(self.SPACE, line) for line in reviews]\n",
    "\n",
    "        return reviews\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        ## Creating dataframes from the list data.  The reviews are arranged in the order that the first 12,500 belong to positive reviews and the rest 12,500 belong to negative reviews.\n",
    "        df_train_reviews_clean = pd.DataFrame(self.reviews_train_clean, columns=['reviews'])\n",
    "        df_train_reviews_clean['target'] = np.where(df_train_reviews_clean.index < 12500, 1, 0)\n",
    "\n",
    "        df_test_reviews_clean = pd.DataFrame(self.reviews_test_clean, columns=['reviews'])\n",
    "        df_test_reviews_clean['target'] = np.where(df_test_reviews_clean.index < 12500, 1, 0)\n",
    "\n",
    "        # Shuffling the rows in both the train and test data.  This is very important before using the data for training.\n",
    "        df_train_reviews_clean = df_train_reviews_clean.sample(frac=1).reset_index(drop=True)\n",
    "        df_test_reviews_clean = df_test_reviews_clean.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # breaking the train data into training and validation\n",
    "        df_train, df_valid = train_test_split(df_train_reviews_clean, test_size=0.25,\n",
    "                                              stratify=df_train_reviews_clean['target'])\n",
    "\n",
    "        self.train = df_train.reset_index(drop=True)\n",
    "        self.val = df_valid.reset_index(drop=True)\n",
    "        self.test = df_test_reviews_clean\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(ImdbDataset(notes=self.train['reviews'],\n",
    "                                      targets=self.train['target'],\n",
    "                                      tokenizer=self.tokenizer,\n",
    "                                      max_len=self.hparams.max_len\n",
    "                                      ),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(ImdbDataset(notes=self.val['reviews'],\n",
    "                                      targets=self.val['target'],\n",
    "                                      tokenizer=self.tokenizer,\n",
    "                                      max_len=self.hparams.max_len),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(ImdbDataset(notes=self.test['reviews'],\n",
    "                                      targets=self.test['target'],\n",
    "                                      tokenizer=self.tokenizer,\n",
    "                                      max_len=self.hparams.max_len),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## The main Pytorch Lightning module\n",
    "class ImdbModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate: float = 0.0001 * 8,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.num_labels = 2\n",
    "        config = transformers.DistilBertConfig(dropout=0.1, attention_dropout=0.2, n_layers=1, n_heads=2)\n",
    "        self.bert = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "        self.pre_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels)\n",
    "        self.dropout = torch.nn.Dropout(self.bert.config.seq_classif_dropout)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "\n",
    "        hidden_state = outputs[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.relu(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def get_outputs(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        hidden_state = outputs[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        return pooled_output\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # batch\n",
    "        input_ids = batch['input_ids']\n",
    "        label = batch['label']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #token_type_ids = batch['token_type_ids']\n",
    "        # fwd\n",
    "        y_hat = self(input_ids, attention_mask, label)\n",
    "\n",
    "        # loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(y_hat.view(-1, self.num_labels), label.view(-1))\n",
    "        #loss = F.cross_entropy(y_hat, label)\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'train_loss': loss, 'learn_rate': self.optim.param_groups[0]['lr']}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # batch\n",
    "        input_ids = batch['input_ids']\n",
    "        label = batch['label']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #token_type_ids = batch['token_type_ids']\n",
    "        # fwd\n",
    "        y_hat = self(input_ids, attention_mask, label)\n",
    "\n",
    "        # loss\n",
    "        #loss = F.cross_entropy(y_hat, label)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(y_hat.view(-1, self.num_labels), label.view(-1))\n",
    "\n",
    "        # acc\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        val_acc = accuracy_score(y_hat.cpu(), label.cpu())\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'val_loss': loss, 'val_acc': val_acc}\n",
    "        # can't log in validation step lossess, accuracy.  It wouldn't log it at every validation step\n",
    "        return {'val_loss': loss, 'val_acc': val_acc, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'val_loss': avg_loss, 'val_acc': avg_val_acc}\n",
    "        return {'val_loss': avg_loss, 'progress_bar': tensorboard_logs, 'log': tensorboard_logs}\n",
    "\n",
    "    def on_batch_end(self):\n",
    "        #for group in self.optim.param_groups:\n",
    "        #    print('learning rate', group['lr'])\n",
    "        # This is needed to use the One Cycle learning rate that needs the learning rate to change after every batch\n",
    "        # Without this, the learning rate will only change after every epoch\n",
    "        if self.sched is not None:\n",
    "            self.sched.step()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.sched is not None:\n",
    "            self.sched.step()\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        input_ids = batch['input_ids']\n",
    "        label = batch['label']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #token_type_ids = batch['token_type_ids']\n",
    "        y_hat = self(input_ids, attention_mask, label)\n",
    "\n",
    "        # loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(y_hat.view(-1, self.num_labels), label.view(-1))\n",
    "\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        test_acc = accuracy_score(y_hat.cpu(), label.cpu())\n",
    "\n",
    "        return {'test_loss': loss, 'test_acc': torch.tensor(test_acc)}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_test_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {'avg_test_loss': avg_loss, 'avg_test_acc': avg_test_acc}\n",
    "        return {'avg_test_acc': avg_test_acc, 'log': tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    # ---------------------\n",
    "    # TRAINING SETUP\n",
    "    # ---------------------\n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        # (LBFGS it is automatically supported, no need for closure function)\n",
    "        optimizer = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.hparams.learning_rate,\n",
    "                                     eps=1e-08)\n",
    "        #scheduler = StepLR(optimizer, step_size=1, gamma=0.2)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-5, total_steps=2000)\n",
    "        #scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-7, max_lr=1e-4, cycle_momentum=False,step_size_up=300)\n",
    "\n",
    "        #scheduler = ReduceLROnPlateau(optimizer, patience=0, factor=0.2)\n",
    "        self.sched = scheduler\n",
    "        self.optim = optimizer\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:307: LightningDeprecationWarning: The `LightningModule.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `LightningModule.on_<train/validation/test>_epoch_end` instead.\n",
      "  rank_zero_deprecation(\n",
      "/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/torch/cuda/__init__.py:146: UserWarning: \n",
      "NVIDIA RTX A5000 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA RTX A5000 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type            | Params\n",
      "---------------------------------------------------\n",
      "0 | bert           | DistilBertModel | 30.9 M\n",
      "1 | pre_classifier | Linear          | 590 K \n",
      "2 | classifier     | Linear          | 1.5 K \n",
      "3 | dropout        | Dropout         | 0     \n",
      "4 | relu           | ReLU            | 0     \n",
      "---------------------------------------------------\n",
      "31.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.5 M    Total params\n",
      "126.063   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000007vscode-remote?line=26'>27</a>\u001b[0m datamodule\u001b[39m.\u001b[39mprepare_data()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000007vscode-remote?line=27'>28</a>\u001b[0m datamodule\u001b[39m.\u001b[39msetup()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000007vscode-remote?line=28'>29</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdatamodule)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 770\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    771\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    772\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    722\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 723\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    724\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:811\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    807\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    808\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    809\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    810\u001b[0m )\n\u001b[0;32m--> 811\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    813\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    814\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1236\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1238\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1323\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1345\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1344\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1347\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1348\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1406\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1405\u001b[0m \u001b[39m# reload dataloaders\u001b[39;00m\n\u001b[0;32m-> 1406\u001b[0m val_loop\u001b[39m.\u001b[39;49m_reload_evaluation_dataloaders()\n\u001b[1;32m   1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_sanity_val_batches \u001b[39m=\u001b[39m [\n\u001b[1;32m   1408\u001b[0m     \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_sanity_val_steps, val_batches) \u001b[39mfor\u001b[39;00m val_batches \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_val_batches\n\u001b[1;32m   1409\u001b[0m ]\n\u001b[1;32m   1411\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:242\u001b[0m, in \u001b[0;36mEvaluationLoop._reload_evaluation_dataloaders\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m     dataloaders \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtest_dataloaders\n\u001b[1;32m    241\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mval_dataloaders \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39m_should_reload_val_dl:\n\u001b[0;32m--> 242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mreset_val_dataloader()\n\u001b[1;32m    243\u001b[0m     dataloaders \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mval_dataloaders\n\u001b[1;32m    244\u001b[0m \u001b[39mif\u001b[39;00m dataloaders \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1965\u001b[0m, in \u001b[0;36mTrainer.reset_val_dataloader\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1963\u001b[0m enable_validation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlimit_val_batches \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   1964\u001b[0m \u001b[39mif\u001b[39;00m source\u001b[39m.\u001b[39mis_defined() \u001b[39mand\u001b[39;00m has_step \u001b[39mand\u001b[39;00m enable_validation:\n\u001b[0;32m-> 1965\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_val_batches, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_dataloaders \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_connector\u001b[39m.\u001b[39;49m_reset_eval_dataloader(\n\u001b[1;32m   1966\u001b[0m         RunningStage\u001b[39m.\u001b[39;49mVALIDATING, model\u001b[39m=\u001b[39;49mpl_module\n\u001b[1;32m   1967\u001b[0m     )\n\u001b[1;32m   1969\u001b[0m     \u001b[39m# store epoch of dataloader reset for reload_dataloaders_every_n_epochs\u001b[39;00m\n\u001b[1;32m   1970\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_val_dl_reload_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_epoch\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:403\u001b[0m, in \u001b[0;36mDataConnector._reset_eval_dataloader\u001b[0;34m(self, mode, model)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dataloaders) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    401\u001b[0m     \u001b[39mfor\u001b[39;00m i, dataloader \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloaders):\n\u001b[1;32m    402\u001b[0m         orig_num_batches \u001b[39m=\u001b[39m num_batches \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 403\u001b[0m             \u001b[39mlen\u001b[39m(dataloader) \u001b[39mif\u001b[39;00m has_len_all_ranks(dataloader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mstrategy, module) \u001b[39melse\u001b[39;00m \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    406\u001b[0m         \u001b[39mif\u001b[39;00m orig_num_batches \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    407\u001b[0m             loader_num_batches\u001b[39m.\u001b[39mappend(orig_num_batches)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/pl/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:126\u001b[0m, in \u001b[0;36mhas_len_all_ranks\u001b[0;34m(dataloader, training_type, model)\u001b[0m\n\u001b[1;32m    123\u001b[0m local_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n\u001b[1;32m    124\u001b[0m total_length \u001b[39m=\u001b[39m training_type\u001b[39m.\u001b[39mreduce(torch\u001b[39m.\u001b[39mtensor(local_length)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice), reduce_op\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m \u001b[39mif\u001b[39;00m total_length \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m:\n\u001b[1;32m    127\u001b[0m     rank_zero_warn(\n\u001b[1;32m    128\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal length of `\u001b[39m\u001b[39m{\u001b[39;00mdataloader\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` across ranks is zero.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Please make sure this was your intention.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m total_length \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m local_length \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "tb_logger = pl_loggers.TensorBoardLogger('../.cache/logs/')\n",
    "\n",
    "# callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode='min',\n",
    "    strict=True,\n",
    ")\n",
    "\n",
    "#print ('inside checkpoint loop')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    #filepath='best_model_{epoch:02d}-{val_loss:.2f}',\n",
    "    dirpath='../.cache/best_model',\n",
    "    verbose=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "lr_logger = LearningRateMonitor(logging_interval='epoch')\n",
    "trainer = Trainer(logger=tb_logger, callbacks=[checkpoint, lr_logger, early_stop], accelerator='gpu', devices=1, max_epochs=10)\n",
    "\n",
    "model = ImdbModel()\n",
    "datamodule = DataModule('../data/imdb/movie_data', batch_size=12)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a530f459ba6cd60adca24892bd2847578451366d11b8bf0a570eeacab4bc5b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/kswamy15/pytorch-lightning-imdb-bert/blob/master/Bert_NLP_Pytorch_IMDB_v3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, random_split\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "import transformers\n",
    "from nlp import load_dataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from argparse import ArgumentParser\n",
    "import re\n",
    "from typing import Optional\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# custom dataset uses Bert Tokenizer to create the Pytorch Dataset\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, notes, targets, tokenizer, max_len):\n",
    "        self.notes = notes\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.notes))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        note = str(self.notes[idx])\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            note,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            #'text': note,\n",
    "            'label': torch.tensor(target, dtype=torch.long),\n",
    "            'input_ids': (encoding['input_ids']).flatten(),\n",
    "            'attention_mask': (encoding['attention_mask']).flatten(),\n",
    "            'token_type_ids': (encoding['token_type_ids']).flatten()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t = transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: str,\n",
    "                 tokenizer: str = 'bert-base-uncased',\n",
    "                 max_len: int = 500,\n",
    "                 batch_size: int = 64,\n",
    "                 num_workers: int = 4,\n",
    "                 *args,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.tokenizer = transformers.BertTokenizerFast.from_pretrained(self.hparams.tokenizer)\n",
    "\n",
    "        self.REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "        self.NO_SPACE = \"\"\n",
    "        self.SPACE = \" \"\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        ## Creates a list of reviews from the big text files containing all the reviews\n",
    "        ## This code was taken from here https://github.com/aaronkub/machine-learning-examples/blob/master/imdb-sentiment-analysis/Sentiment%20Analysis%20Walkthrough%20Part%201.ipynb\n",
    "        reviews_train = []\n",
    "        DIR = Path(self.hparams.data_dir)\n",
    "        with open(DIR / 'full_train.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                reviews_train.append(line.strip())\n",
    "\n",
    "        reviews_test = []\n",
    "        with open(DIR / 'full_test.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                reviews_test.append(line.strip())\n",
    "\n",
    "        self.reviews_train_clean = self.preprocess_reviews(reviews_train)\n",
    "        self.reviews_test_clean = self.preprocess_reviews(reviews_test)\n",
    "\n",
    "    def preprocess_reviews(self, reviews):\n",
    "        #reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "        reviews = [self.REPLACE_WITH_SPACE.sub(self.SPACE, line) for line in reviews]\n",
    "\n",
    "        return reviews\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        ## Creating dataframes from the list data.  The reviews are arranged in the order that the first 12,500 belong to positive reviews and the rest 12,500 belong to negative reviews.\n",
    "        df_train_reviews_clean = pd.DataFrame(self.reviews_train_clean, columns=['reviews'])\n",
    "        df_train_reviews_clean['target'] = np.where(df_train_reviews_clean.index < 12500, 1, 0)\n",
    "\n",
    "        df_test_reviews_clean = pd.DataFrame(self.reviews_test_clean, columns=['reviews'])\n",
    "        df_test_reviews_clean['target'] = np.where(df_test_reviews_clean.index < 12500, 1, 0)\n",
    "\n",
    "        # Shuffling the rows in both the train and test data.  This is very important before using the data for training.\n",
    "        df_train_reviews_clean = df_train_reviews_clean.sample(frac=1).reset_index(drop=True)\n",
    "        df_test_reviews_clean = df_test_reviews_clean.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # breaking the train data into training and validation\n",
    "        df_train, df_valid = train_test_split(df_train_reviews_clean, test_size=0.25,\n",
    "                                              stratify=df_train_reviews_clean['target'])\n",
    "\n",
    "        self.train = df_train.reset_index(drop=True)\n",
    "        self.val = df_valid.reset_index(drop=True)\n",
    "        self.test = df_test_reviews_clean\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(ImdbDataset(notes=self.train['reviews'],\n",
    "                                      targets=self.train['target'],\n",
    "                                      tokenizer=self.tokenizer,\n",
    "                                      max_len=self.hparams.max_len\n",
    "                                      ),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(ImdbDataset(notes=self.val['reviews'],\n",
    "                                      targets=self.val['target'],\n",
    "                                      tokenizer=self.tokenizer,\n",
    "                                      max_len=self.hparams.max_len),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(ImdbDataset(notes=self.test['reviews'],\n",
    "                                      targets=self.test['target'],\n",
    "                                      tokenizer=self.tokenizer,\n",
    "                                      max_len=self.hparams.max_len),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## The main Pytorch Lightning module\n",
    "class ImdbModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate: float = 0.0001 * 8,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.num_labels = 2\n",
    "        config = transformers.DistilBertConfig(dropout=0.1, attention_dropout=0.2)\n",
    "        self.bert = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "        self.pre_classifier = torch.nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, self.num_labels)\n",
    "        self.dropout = torch.nn.Dropout(self.bert.config.seq_classif_dropout)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "\n",
    "        hidden_state = outputs[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.relu(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def get_outputs(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        hidden_state = outputs[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        return pooled_output\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # batch\n",
    "        input_ids = batch['input_ids']\n",
    "        label = batch['label']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #token_type_ids = batch['token_type_ids']\n",
    "        # fwd\n",
    "        y_hat = self(input_ids, attention_mask, label)\n",
    "\n",
    "        # loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(y_hat.view(-1, self.num_labels), label.view(-1))\n",
    "        #loss = F.cross_entropy(y_hat, label)\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'train_loss': loss, 'learn_rate': self.optim.param_groups[0]['lr']}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # batch\n",
    "        input_ids = batch['input_ids']\n",
    "        label = batch['label']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #token_type_ids = batch['token_type_ids']\n",
    "        # fwd\n",
    "        y_hat = self(input_ids, attention_mask, label)\n",
    "\n",
    "        # loss\n",
    "        #loss = F.cross_entropy(y_hat, label)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(y_hat.view(-1, self.num_labels), label.view(-1))\n",
    "\n",
    "        # acc\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        val_acc = accuracy_score(y_hat.cpu(), label.cpu())\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'val_loss': loss, 'val_acc': val_acc}\n",
    "        # can't log in validation step lossess, accuracy.  It wouldn't log it at every validation step\n",
    "        return {'val_loss': loss, 'val_acc': val_acc, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'val_loss': avg_loss, 'val_acc': avg_val_acc}\n",
    "        return {'val_loss': avg_loss, 'progress_bar': tensorboard_logs, 'log': tensorboard_logs}\n",
    "\n",
    "    def on_batch_end(self):\n",
    "        #for group in self.optim.param_groups:\n",
    "        #    print('learning rate', group['lr'])\n",
    "        # This is needed to use the One Cycle learning rate that needs the learning rate to change after every batch\n",
    "        # Without this, the learning rate will only change after every epoch\n",
    "        if self.sched is not None:\n",
    "            self.sched.step()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.sched is not None:\n",
    "            self.sched.step()\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        input_ids = batch['input_ids']\n",
    "        label = batch['label']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        #token_type_ids = batch['token_type_ids']\n",
    "        y_hat = self(input_ids, attention_mask, label)\n",
    "\n",
    "        # loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(y_hat.view(-1, self.num_labels), label.view(-1))\n",
    "\n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        test_acc = accuracy_score(y_hat.cpu(), label.cpu())\n",
    "\n",
    "        return {'test_loss': loss, 'test_acc': torch.tensor(test_acc)}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_test_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {'avg_test_loss': avg_loss, 'avg_test_acc': avg_test_acc}\n",
    "        return {'avg_test_acc': avg_test_acc, 'log': tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    # ---------------------\n",
    "    # TRAINING SETUP\n",
    "    # ---------------------\n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        # (LBFGS it is automatically supported, no need for closure function)\n",
    "        optimizer = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.hparams.learning_rate,\n",
    "                                     eps=1e-08)\n",
    "        #scheduler = StepLR(optimizer, step_size=1, gamma=0.2)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-5, total_steps=2000)\n",
    "        #scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-7, max_lr=1e-4, cycle_momentum=False,step_size_up=300)\n",
    "\n",
    "        #scheduler = ReduceLROnPlateau(optimizer, patience=0, factor=0.2)\n",
    "        self.sched = scheduler\n",
    "        self.optim = optimizer\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:307: LightningDeprecationWarning: The `LightningModule.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `LightningModule.on_<train/validation/test>_epoch_end` instead.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type            | Params\n",
      "---------------------------------------------------\n",
      "0 | bert           | DistilBertModel | 66.4 M\n",
      "1 | pre_classifier | Linear          | 590 K \n",
      "2 | classifier     | Linear          | 1.5 K \n",
      "3 | dropout        | Dropout         | 0     \n",
      "4 | relu           | ReLU            | 0     \n",
      "---------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 391/391 [05:05<00:00,  1.28it/s, loss=0.476, v_num=4]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Early stopping conditioned on metric `val_loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: ``",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000007vscode-remote?line=26'>27</a>\u001b[0m datamodule\u001b[39m.\u001b[39mprepare_data()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000007vscode-remote?line=27'>28</a>\u001b[0m datamodule\u001b[39m.\u001b[39msetup()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22475055227d/root/lightning-hydra-template/notebooks/demo_Bert_NLP_Pytorch_IMDB_v3.ipynb#ch0000007vscode-remote?line=28'>29</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdatamodule)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 770\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    771\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    772\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    722\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 723\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    724\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:811\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    807\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    808\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    809\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    810\u001b[0m )\n\u001b[0;32m--> 811\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    813\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    814\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1236\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1238\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1323\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1353\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1352\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:205\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:297\u001b[0m, in \u001b[0;36mFitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    296\u001b[0m \u001b[39m# call train epoch end hooks\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_callback_hooks(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    298\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    300\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_epoch_end\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1636\u001b[0m, in \u001b[0;36mTrainer._call_callback_hooks\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1634\u001b[0m         \u001b[39mif\u001b[39;00m callable(fn):\n\u001b[1;32m   1635\u001b[0m             \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Callback]\u001b[39m\u001b[39m{\u001b[39;00mcallback\u001b[39m.\u001b[39mstate_key\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1636\u001b[0m                 fn(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1638\u001b[0m \u001b[39mif\u001b[39;00m pl_module:\n\u001b[1;32m   1639\u001b[0m     \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1640\u001b[0m     pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/callbacks/early_stopping.py:179\u001b[0m, in \u001b[0;36mEarlyStopping.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_on_train_epoch_end \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_skip_check(trainer):\n\u001b[1;32m    178\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_early_stopping_check(trainer)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/callbacks/early_stopping.py:190\u001b[0m, in \u001b[0;36mEarlyStopping._run_early_stopping_check\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39m\"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m logs \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mcallback_metrics\n\u001b[0;32m--> 190\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mfast_dev_run \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_condition_metric(  \u001b[39m# disable early_stopping with fast_dev_run\u001b[39;49;00m\n\u001b[1;32m    191\u001b[0m     logs\n\u001b[1;32m    192\u001b[0m ):  \u001b[39m# short circuit if metric not present\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    195\u001b[0m current \u001b[39m=\u001b[39m logs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmonitor]\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/zsl/lib/python3.8/site-packages/pytorch_lightning/callbacks/early_stopping.py:145\u001b[0m, in \u001b[0;36mEarlyStopping._validate_condition_metric\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m monitor_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrict:\n\u001b[0;32m--> 145\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    146\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    147\u001b[0m         rank_zero_warn(error_msg, category\u001b[39m=\u001b[39m\u001b[39mRuntimeWarning\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Early stopping conditioned on metric `val_loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: ``"
     ]
    }
   ],
   "source": [
    "tb_logger = pl_loggers.TensorBoardLogger('../.cache/logs/')\n",
    "\n",
    "# callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    mode='min',\n",
    "    strict=True,\n",
    ")\n",
    "\n",
    "#print ('inside checkpoint loop')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    #filepath='best_model_{epoch:02d}-{val_loss:.2f}',\n",
    "    dirpath='../.cache/best_model',\n",
    "    verbose=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "lr_logger = LearningRateMonitor(logging_interval='epoch')\n",
    "trainer = Trainer(logger=tb_logger, callbacks=[checkpoint, lr_logger, early_stop], accelerator='gpu', devices=1, max_epochs=3)\n",
    "\n",
    "model = ImdbModel()\n",
    "datamodule = DataModule('../data/imdb/movie_data', batch_size=64)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('zsl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1efd3c77b126e853d7107796dada0eb58262837b04201097653767f94791c600"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
